{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN,OPTICS,MeanShift\n",
    "import os, shutil, glob, os.path\n",
    "from PIL import Image as pil_image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "#importing the header files \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA \n",
    "import librosa\n",
    "from sklearn import mixture\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa.display\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from kneed import DataGenerator, KneeLocator\n",
    "%matplotlib inline\n",
    "import os, shutil, glob, os.path\n",
    "from PIL import Image as pil_image\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import hdbscan\n",
    "from numpy import load,save\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filepaths():\n",
    "    imdir_ideology = 'ideology_image_dataset/'\n",
    "    imdir_muslim='muslim_image_dataset/'\n",
    "    ideology_files=os.listdir('ideology_image_dataset/')\n",
    "    muslim_files=os.listdir('muslim_image_dataset/')\n",
    "    len(ideology_files),len(muslim_files)\n",
    "\n",
    "    ideology_files_path=[os.path.join(imdir_ideology,file) for file in ideology_files ]\n",
    "    muslim_files_path=[os.path.join(imdir_muslim,file) for file in muslim_files]\n",
    "    return ideology_files_path,muslim_files_path\n",
    "\n",
    "def loadFeatures(filename):\n",
    "    print(\"Loading file : \",filename)\n",
    "    features= load(filename)\n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGridSearch(estimator,params_dict,train_data):\n",
    "    \n",
    "    cv = [(slice(None), slice(None))]\n",
    "    gs = GridSearchCV(estimator=estimator, param_grid=params_dict, scoring=cv_silhouette_scorer, cv=cv, n_jobs=-1)\n",
    "    gs.fit(train_data)\n",
    "    try:\n",
    "        predicted_labels= gs.best_estimator_.labels_\n",
    "    except:\n",
    "        predicted_labels=gs.predict(train_data)\n",
    "    \n",
    "    print(\"best estimator is \",gs.best_estimator_)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_silhouette_scorer(estimator, X):\n",
    "    estimator.fit(X)\n",
    "    try:\n",
    "        cluster_labels = estimator.labels_\n",
    "    except Exception as e:\n",
    "      #  print(e,estimator)\n",
    "        cluster_labels=estimator.predict(X)\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    no_noise_labels=num_labels\n",
    "    if(set(cluster_labels).issuperset({-1})):\n",
    "        n_clustersLen=len(set(cluster_labels))-1\n",
    "        no_noise_labels=n_clustersLen\n",
    "    \n",
    "    num_samples = len(X)\n",
    "    if num_labels == 1:\n",
    "        return -1\n",
    "    elif num_labels==num_samples:\n",
    "        return -1\n",
    "    elif no_noise_labels==1:\n",
    "        return -1\n",
    "    else:\n",
    "        return metrics.silhouette_score(X, cluster_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_Score(features,y_pred,output_df,model):\n",
    "    try:\n",
    "        \n",
    "        num_labels=len(set(y_pred))\n",
    "        total_samples=len(y_pred)\n",
    "        if(num_labels==1 or num_labels==total_samples):\n",
    "            output_df.loc[model,'silhouette'] =-1\n",
    "            output_df.loc[model,'calinski'] =-1\n",
    "            output_df.loc[model,'davies'] =-1\n",
    "            \n",
    "        else:\n",
    "            output_df.loc[model,'silhouette'] =metrics.silhouette_score(features,y_pred)\n",
    "            output_df.loc[model,'calinski'] =metrics.calinski_harabasz_score(features, y_pred)\n",
    "            output_df.loc[model,'davies'] =metrics.davies_bouldin_score(features,y_pred)\n",
    "            \n",
    "    \n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    return output_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEpsilon(train_data):\n",
    "    neigh = sklearn.neighbors.NearestNeighbors(n_neighbors=4)\n",
    "    nbrs = neigh.fit(train_data)\n",
    "    distances, indices = nbrs.kneighbors(train_data)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1]\n",
    "    y=distances\n",
    "    x=list(np.arange(0,len(distances)))\n",
    "    sensitivity = [1,3, 5, 10, 20,40,60,80, 100, 120,150,180,200,250,300,350,400]\n",
    "    epsilons=[]\n",
    "    for s in sensitivity:\n",
    "        try:\n",
    "            kneedle = KneeLocator(x,y,S=s, curve='convex', direction='increasing')\n",
    "            epsilon=kneedle.all_elbows_y[0]\n",
    "            if(len(epsilons)>=1 and epsilons[-1]-epsilon<=0.001):\n",
    "                print(\"\")\n",
    "                \n",
    "            else:    \n",
    "                epsilons.append(epsilon)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if(len(epsilons)>=1):\n",
    "                epsilons.append(epsilons[-1]+s/10)\n",
    "            else:\n",
    "                epsilons.append(s/10)\n",
    "    \n",
    "   # epsilons.append(0.3)\n",
    "   # epsilons.append(0.5)\n",
    "   # epsilons.append(0.8)\n",
    "        \n",
    "    print(epsilons)\n",
    " \n",
    "    return epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def runModels(train_data,output_df,filename):\n",
    "   \n",
    "    print(\"HDBSCAN\")\n",
    "    \n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=2,cluster_selection_epsilon= 0.01,min_samples= 1)\n",
    "    predicted_labels = clusterer.fit_predict(features)\n",
    "    if(set(predicted_labels).issuperset({-1})):\n",
    "        n_clustersLen=len(set(predicted_labels))-1\n",
    "    else:\n",
    "        n_clustersLen=len(set(predicted_labels))\n",
    "        \n",
    "    evaluation_Score(train_data,predicted_labels,output_df,'HDBSCAN')\n",
    "    output_df.loc['HDBSCAN','n_clusters']=n_clustersLen\n",
    "          \n",
    "    \n",
    "    np.save(filename+'_hdbscan_labels.npy', np.array(predicted_labels))\n",
    "    print(output_df)\n",
    "    \n",
    "   \n",
    "    \n",
    "    print(\"DBSCAN\")\n",
    "    epsilons=getEpsilon(train_data)\n",
    "    params_dict = {'eps':epsilons,'min_samples':[2,3,4],'metric':['euclidean','manhattan','mahalanobis']}\n",
    "    predicted_labels=runGridSearch(sklearn.cluster.DBSCAN(),params_dict,train_data)\n",
    "    \n",
    "    evaluation_Score(train_data,predicted_labels,output_df,'DBSCAN')\n",
    "    if(set(predicted_labels).issuperset({-1})):\n",
    "        n_clustersLen=len(set(predicted_labels))-1\n",
    "    else:\n",
    "        n_clustersLen=len(set(predicted_labels))\n",
    "    output_df.loc['DBSCAN','n_clusters']=n_clustersLen\n",
    "    print(output_df)\n",
    "    np.save(filename+'_dbscan_labels.npy', np.array(predicted_labels))\n",
    "    \n",
    "    \n",
    "    print(\"Mean shift\")\n",
    "    quantiles=[0.2,0.5,0.8,1]\n",
    "    params_dict={}\n",
    "    params_dict['bandwidth']=[]\n",
    "    for quantile in quantiles:\n",
    "        params_dict['bandwidth'].append(sklearn.cluster.estimate_bandwidth(train_data, quantile=quantile, n_samples=500))\n",
    "\n",
    "    params_dict['bandwidth'].append(0.2)\n",
    "    predicted_labels=runGridSearch(sklearn.cluster.MeanShift(),params_dict,train_data)\n",
    "    evaluation_Score(train_data,predicted_labels,output_df,'Mean-shift')\n",
    "    if(set(predicted_labels).issuperset({-1})):\n",
    "        n_clustersLen=len(set(predicted_labels))-1\n",
    "    else:\n",
    "        n_clustersLen=len(set(predicted_labels))\n",
    "    output_df.loc['Mean-shift','n_clusters']=n_clustersLen\n",
    "    \n",
    "    np.save(filename+'_mean_shift_labels.npy', np.array(predicted_labels))\n",
    "            \n",
    "    print(\"Optics\")\n",
    "\n",
    "    params_dict = {'min_samples':[3,4,5],'metric':['euclidean','manhattan','mahalanobis']}\n",
    "    predicted_labels=runGridSearch(sklearn.cluster.OPTICS(),params_dict,train_data)\n",
    "    if(set(predicted_labels).issuperset({-1})):\n",
    "        n_clustersLen=len(set(predicted_labels))-1\n",
    "    else:\n",
    "        n_clustersLen=len(set(predicted_labels))\n",
    "    evaluation_Score(train_data,predicted_labels,output_df,'Optics')\n",
    "    output_df.loc['Optics','n_clusters']=n_clustersLen\n",
    "    \n",
    "    print(output_df)\n",
    "    \n",
    "    np.save(filename+'_optics_labels.npy', np.array(predicted_labels))\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "    return output_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runClustering(train_data,filename):\n",
    "   \n",
    "    \n",
    "   # output_df = pd.DataFrame(index=['DBSCAN'],columns=['n_clusters','silhouette','calinski','davies'])\n",
    "    output_df = pd.DataFrame(index=['HDBSCAN''DBSCAN','Mean-shift','Optics'],columns=['n_clusters','silhouette','calinski'])\n",
    "   \n",
    "    output_df=runModels(train_data,output_df,filename)\n",
    "    \n",
    " \n",
    "   \n",
    "    return output_df\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ideology_files_path,muslim_files_path=load_filepaths()\n",
    "    for file in os.listdir('Image_features/')[0:1]:\n",
    "        if file.endswith('.npy'):\n",
    "            print(f'Image_features/{file}')\n",
    "            features=loadFeatures(os.path.join('Image_features',file))\n",
    "            print(\"tsne....\")\n",
    "            tsne_transformed=TSNE(n_components=3, n_jobs=-1).fit_transform(features)\n",
    "            output_df=runClustering(tsne_transformed,f'image-results-updateGrid/{file}-tsne')\n",
    "            output_df.to_csv(f'image-results-updateGrid/{file}-tsne.csv')\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(\"pca....\")\n",
    "            pca_dims = PCA().fit(features)\n",
    "            cumsum = np.cumsum(pca_dims.explained_variance_ratio_)\n",
    "            d = np.argmax(cumsum >= 0.95) + 1\n",
    "            print(d)\n",
    "            if(d==1):\n",
    "                d=d+1\n",
    "            pca_transformed=PCA(n_components=d).fit_transform(features)\n",
    "            output_df=runClustering(pca_transformed,f'image-results-updateGrid/{file}-pca')\n",
    "            output_df.to_csv(f'image-results-updateGrid/{file}-pca.csv')\n",
    "            \n",
    "            print(\"umap....\")\n",
    "            reducer = umap.UMAP(random_state=42,n_components=2)\n",
    "            umap_transformed = reducer.fit_transform(features)\n",
    "            output_df=runClustering(umap_transformed,f'image-results-updateGrid/{file}-umap')\n",
    "            output_df.to_csv(f'image-results-updateGrid/{file}-umap.csv')\n",
    "            \n",
    "            \n",
    "            pca_50 = PCA(n_components=50)\n",
    "            pca_result_50 = pca_50.fit_transform(features)\n",
    "            tsne_transformed = TSNE(random_state = 42, n_components=3,verbose=0, perplexity=40, n_iter=400).fit_transform(pca_result_50)\n",
    "            output_df=runClustering(tsne_transformed,f'image-results-updateGrid/{file}-pca-tsne')\n",
    "            output_df.to_csv(f'image-results-updateGrid/{file}-pca-tsne.csv')\n",
    "            \n",
    "\n",
    "\n",
    "                        #output_dfs=runClustering(tsne_transformed,f'image-results-updateGrid{file}')\n",
    "\n",
    "\n",
    "            \n",
    "   # pca_transformed=pca_transform(features)\n",
    "   # tsne_transformed=tsne_transform(features)\n",
    "\n",
    "#    output_df1,predicted_labels=runClustering(features,file)\n",
    " #   print(output_df1)\n",
    "   # output_df2,predicted_labels=runClustering(pca_transformed,file)\n",
    "   # print(output_df2)\n",
    "    #output_df3,predicted_labels=runClustering(tsne_transformed,file)\n",
    "    #print(output_df3)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
