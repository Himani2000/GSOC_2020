{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN,OPTICS,MeanShift\n",
    "import os, shutil, glob, os.path\n",
    "from PIL import Image as pil_image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "#importing the header files \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA \n",
    "import librosa\n",
    "from sklearn import mixture\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa.display\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from kneed import DataGenerator, KneeLocator\n",
    "%matplotlib inline\n",
    "import os, shutil, glob, os.path\n",
    "from PIL import Image as pil_image\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from numpy import load,save\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filepaths():\n",
    "    imdir_ideology = 'ideology_image_dataset/'\n",
    "    imdir_muslim='muslim_image_dataset/'\n",
    "    ideology_files=os.listdir('ideology_image_dataset/')\n",
    "    muslim_files=os.listdir('muslim_image_dataset/')\n",
    "    len(ideology_files),len(muslim_files)\n",
    "\n",
    "    ideology_files_path=[os.path.join(imdir_ideology,file) for file in ideology_files ]\n",
    "    muslim_files_path=[os.path.join(imdir_muslim,file) for file in muslim_files]\n",
    "    return ideology_files_path,muslim_files_path\n",
    "\n",
    "def loadFeatures(filename):\n",
    "    print(\"Loading file : \",filename)\n",
    "    features= load(filename)\n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_silhouette_scorer(estimator, X):\n",
    "    estimator.fit(X)\n",
    "    try:\n",
    "        cluster_labels = estimator.labels_\n",
    "    except Exception as e:\n",
    "      #  print(e,estimator)\n",
    "        cluster_labels=estimator.predict(X)\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    no_noise_labels=num_labels\n",
    "    if(set(cluster_labels).issuperset({-1})):\n",
    "        n_clustersLen=len(set(cluster_labels))-1\n",
    "        no_noise_labels=n_clustersLen\n",
    "    \n",
    "    num_samples = len(X)\n",
    "    if num_labels == 1 or num_labels == num_samples or no_noise_labels==1:\n",
    "        return -1\n",
    "    else:\n",
    "        return metrics.silhouette_score(X, cluster_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_Score(features,y_pred,output_df,model):\n",
    "    try:\n",
    "        \n",
    "        num_labels=len(set(y_pred))\n",
    "        total_samples=len(y_pred)\n",
    "        if(num_labels==1 or num_labels==total_samples):\n",
    "            output_df.loc[model,'silhouette'] =-1\n",
    "            output_df.loc[model,'calinski'] =-1\n",
    "            output_df.loc[model,'davies'] =-1\n",
    "            \n",
    "        else:\n",
    "            output_df.loc[model,'silhouette'] =metrics.silhouette_score(features,y_pred)\n",
    "            output_df.loc[model,'calinski'] =metrics.calinski_harabasz_score(features, y_pred)\n",
    "            output_df.loc[model,'davies'] =metrics.davies_bouldin_score(features,y_pred)\n",
    "            \n",
    "    \n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    return output_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGridSearch(estimator,params_dict,train_data):\n",
    "    \n",
    "    cv = [(slice(None), slice(None))]\n",
    "    gs = GridSearchCV(estimator=estimator, param_grid=params_dict, scoring=cv_silhouette_scorer, cv=cv, n_jobs=-1)\n",
    "    gs.fit(train_data)\n",
    "    try:\n",
    "        predicted_labels= gs.best_estimator_.labels_\n",
    "    except:\n",
    "        predicted_labels=gs.predict(train_data)\n",
    "    \n",
    "    print(\"best estimator is \",gs.best_estimator_)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def runModels(train_data,output_df,filename):\n",
    "   \n",
    "   \n",
    "        \n",
    "    print(\"Agglomerative clustering results\")\n",
    "    #knn_graph = kneighbors_graph(train_data, 1, include_self=False)\n",
    "   # grid_graph = grid_to_graph(*train_data.shape)\n",
    "    params_dict={'linkage':['ward','average'],'distance_threshold':[5,6,10,20,30,40,50,60,70],'n_clusters':[None]}\n",
    "    predicted_labels=runGridSearch(sklearn.cluster.AgglomerativeClustering(),params_dict,train_data)\n",
    "    \n",
    "    evaluation_Score(train_data,predicted_labels,output_df,'Agglomerative clustering')\n",
    "    if(set(predicted_labels).issuperset({-1})):\n",
    "        n_clustersLen=len(set(predicted_labels))-1\n",
    "    else:\n",
    "        n_clustersLen=len(set(predicted_labels))\n",
    "    output_df.loc['Agglomerative clustering','n_clusters']=n_clustersLen\n",
    "    \n",
    "    np.save(filename+'_agg_labels.npy', np.array(predicted_labels))\n",
    "    print(output_df)\n",
    "    \n",
    "    print(\"Birch\")\n",
    "    threshold_estimation=train_data.shape[1]/10\n",
    "    if(threshold_estimation-10>=10):\n",
    "        threshold_estimation-=10\n",
    "    params_dict={'threshold':np.arange(threshold_estimation,threshold_estimation+5,0.3).tolist(),'n_clusters':[None]}\n",
    "    predicted_labels=runGridSearch(sklearn.cluster.Birch(),params_dict,train_data)\n",
    "    #print(\"number of distinct labels in the birch os\")\n",
    "    evaluation_Score(train_data,predicted_labels,output_df,'Birch')\n",
    "    if(set(predicted_labels).issuperset({-1})):\n",
    "        n_clustersLen=len(set(predicted_labels))-1\n",
    "    else:\n",
    "        n_clustersLen=len(set(predicted_labels))\n",
    "    output_df.loc['Birch','n_clusters']=n_clustersLen\n",
    "    \n",
    "    np.save(filename+'_birch_labels.npy', np.array(predicted_labels))\n",
    "    print(output_df)\n",
    "    \n",
    "\n",
    "   \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "    return output_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runClustering(train_data,filename):\n",
    "   \n",
    "    output_df = pd.DataFrame(index=['Birch','Agglomerative clustering'],columns=['n_clusters','silhouette','calinski'])\n",
    "   \n",
    "    output_df=runModels(train_data,output_df,filename)\n",
    "    \n",
    " \n",
    "   \n",
    "    return output_df\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ideology_files_path,muslim_files_path=load_filepaths()\n",
    "    for file in os.listdir('Image_features/'):\n",
    "        if file.endswith('.npy'):\n",
    "            print(f'Image_features/{file}')\n",
    "            features=loadFeatures(os.path.join('Image_features',file))\n",
    "            print(\"tsne....\")\n",
    "            tsne_transformed=TSNE(n_components=3, n_jobs=-1).fit_transform(features)\n",
    "            output_df=runClustering(tsne_transformed,f'image-results-hier/{file}-tsne')\n",
    "            output_df.to_csv(f'image-results-hier/{file}-tsne.csv')\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(\"pca....\")\n",
    "            pca_dims = PCA().fit(features)\n",
    "            cumsum = np.cumsum(pca_dims.explained_variance_ratio_)\n",
    "            d = np.argmax(cumsum >= 0.95) + 1\n",
    "            print(d)\n",
    "            if(d==1):\n",
    "                d=d+1\n",
    "            pca_transformed=PCA(n_components=d).fit_transform(features)\n",
    "            output_df=runClustering(pca_transformed,f'image-results-hier/{file}-pca')\n",
    "            output_df.to_csv(f'image-results-hier/{file}-pca.csv')\n",
    "            \n",
    "            print(\"umap....\")\n",
    "            reducer = umap.UMAP(random_state=42,n_components=2)\n",
    "            umap_transformed = reducer.fit_transform(features)\n",
    "            output_df=runClustering(umap_transformed,f'image-results-hier/{file}-umap')\n",
    "            output_df.to_csv(f'image-results-hier/{file}-umap.csv')\n",
    "            \n",
    "            \n",
    "            pca_50 = PCA(n_components=50)\n",
    "            pca_result_50 = pca_50.fit_transform(features)\n",
    "            tsne_transformed = TSNE(random_state = 42, n_components=3,verbose=0, perplexity=40, n_iter=400).fit_transform(pca_result_50)\n",
    "            output_df=runClustering(tsne_transformed,f'image-results-hier/{file}-pca-tsne')\n",
    "            output_df.to_csv(f'image-results-hier/{file}-pca-tsne.csv')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
