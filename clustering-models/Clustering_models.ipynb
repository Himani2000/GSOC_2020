{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the header files \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA \n",
    "import librosa\n",
    "from sklearn import mixture\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa.display\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from kneed import DataGenerator, KneeLocator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_muslim_labels(filename):\n",
    "    df=pd.read_csv(filename)\n",
    "    final_files=list(df['file_name'])\n",
    "    \n",
    "    df=pd.read_excel(\"spreadsheet_data/muslim_concordance_250_annotated.xls\")\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    \n",
    "    final_updated_files=[]\n",
    "    for file in final_files:\n",
    "        for i in range(len(df)):\n",
    "            if(df.loc[i,'File Name']==file):\n",
    "                if(df['u'][i]==1):\n",
    "                    final_updated_files.append('u')\n",
    "    \n",
    "                elif(df['a'][i]==1):\n",
    "                    final_updated_files.append('a')\n",
    "    \n",
    "                elif(df['misaligned/error/etc.'][i]==1):\n",
    "                    final_updated_files.append('misaligned')\n",
    "    \n",
    "                elif(df['some other problem'][i]==1):\n",
    "                    final_updated_files.append('other')\n",
    "    \n",
    "                elif(df['can\\'t decide'][i]==1):\n",
    "                    final_updated_files.append('cantdecide')\n",
    "    \n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(np.array(final_updated_files))\n",
    "\n",
    "    encoded_labels=le.transform(np.array(final_updated_files))\n",
    "    print(\"classes\",le.classes_)\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename,dataset_name):\n",
    "    df=pd.read_csv(filename)\n",
    "    train_data=df.iloc[:,0:-1]\n",
    "    scaler =MinMaxScaler()\n",
    "    train_data = scaler.fit_transform(train_data)\n",
    "    train_data=pd.DataFrame(train_data)\n",
    "    train_data=train_data.fillna(0)\n",
    "\n",
    "    if(dataset_name=='ideology'):\n",
    "        splited_df=df['file_name'].str.split(\"clip_\",expand=True)\n",
    "        labels=splited_df[1].str.split(\"_\",expand=True)\n",
    "        actual_labels=labels[1]\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        le.fit(np.array(actual_labels))\n",
    "        true_labels=le.transform(np.array(actual_labels))\n",
    "        \n",
    "    elif(dataset_name=='muslim'):\n",
    "        true_labels=get_muslim_labels(filename)\n",
    "        \n",
    "\n",
    "   \n",
    "    train_data=train_data.fillna(0)\n",
    "    return train_data,true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_silhouette_scorer(estimator, X):\n",
    "    estimator.fit(X)\n",
    "    try:\n",
    "        cluster_labels = estimator.labels_\n",
    "    except Exception as e:\n",
    "      #  print(e,estimator)\n",
    "        cluster_labels=estimator.predict(X)\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X.index)\n",
    "    if num_labels == 1 or num_labels == num_samples:\n",
    "        return -1\n",
    "    else:\n",
    "        return metrics.silhouette_score(X, cluster_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_Score(y_true,y_pred,output_df,model):\n",
    "    try:\n",
    "        output_df.loc[model,'ARI'] =metrics.adjusted_rand_score(y_true,y_pred)\n",
    "        output_df.loc[model,'MI'] = metrics.adjusted_mutual_info_score(y_true,y_pred)\n",
    "        output_df.loc[model,'H'] = metrics.homogeneity_score(y_true,y_pred)\n",
    "        output_df.loc[model,'C'] = metrics.completeness_score(y_true,y_pred)\n",
    "        output_df.loc[model,'V'] = metrics.v_measure_score(y_true,y_pred)\n",
    "        output_df.loc[model,'FM'] =metrics.fowlkes_mallows_score(y_true,y_pred)\n",
    "        output_df.loc[model,'A']=metrics.accuracy_score(y_true, y_pred)\n",
    "        output_df.loc[model,'R']=metrics.recall_score(y_true,y_pred)\n",
    "        output_df.loc[model,'P']=metrics.precision_score(y_true,y_pred)\n",
    "\n",
    "    except ValueError as e:\n",
    "        output_df.loc[model,'R']=metrics.recall_score(y_true,y_pred,average='micro')\n",
    "        output_df.loc[model,'P']=metrics.precision_score(y_true,y_pred,average='micro')\n",
    "\n",
    "        \n",
    "    \n",
    "    return output_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEpsilon(train_data):\n",
    "    neigh = sklearn.neighbors.NearestNeighbors(n_neighbors=2)\n",
    "    nbrs = neigh.fit(train_data)\n",
    "    distances, indices = nbrs.kneighbors(train_data)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1]\n",
    "    y=distances\n",
    "    x=list(np.arange(0,len(distances)))\n",
    "\n",
    "    kneedle = KneeLocator(x,y,S=10, curve='convex', direction='increasing')\n",
    "    epsilon=kneedle.all_elbows_y[0]\n",
    "    epsilons=[]\n",
    "    for i in range(3):\n",
    "        epsilons.append(epsilon+i/10)\n",
    "    return epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGridSearch(estimator,params_dict,train_data):\n",
    "    \n",
    "    cv = [(slice(None), slice(None))]\n",
    "    gs = GridSearchCV(estimator=estimator, param_grid=params_dict, scoring=cv_silhouette_scorer, cv=cv, n_jobs=-1)\n",
    "    gs.fit(train_data)\n",
    "    try:\n",
    "        predicted_labels= gs.best_estimator_.labels_\n",
    "    except:\n",
    "        predicted_labels=gs.predict(train_data)\n",
    "    \n",
    "    \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModels(train_data,true_labels,output_df,dataset_name):\n",
    "   # output_df = pd.DataFrame(index=['K-Means','Agglomerative clustering','Birch','DBSCAN','Mean-shift','Optics','Gaussian-mixture'],columns=['n_clusters','ARI','MI','H','C','V','FM','A','R','P'])\n",
    "    if(dataset_name=='ideology'):\n",
    "        n_clusters=[2]\n",
    "    elif(dataset_name=='muslim'):\n",
    "        n_clusters=[5]\n",
    "    \n",
    "    print(\"K-means results\")\n",
    "    params_dict={'n_clusters':n_clusters,'init':['k-means++'],'n_init':[200], 'max_iter':[1000000],'algorithm':['auto','full', 'elkan']}\n",
    "    #params_dict={'n_clusters':[2],'init':['k-means++'],'n_init':[200], 'max_iter':[1000000],'algorithm':['auto']}\n",
    "    predicted_labels=runGridSearch(sklearn.cluster.KMeans(),params_dict,train_data)\n",
    "    evaluation_Score(true_labels,predicted_labels,output_df,'K-Means')\n",
    "    output_df.loc['K-Means','n_clusters']=len(set(predicted_labels))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Agglomerative clustering results\")\n",
    "    params_dict={'affinity':['euclidean'], 'linkage':['ward','complete','average','single']}\n",
    "    #params_dict={'affinity':['euclidean'], 'linkage':['ward']}\n",
    "    predicted_labels=runGridSearch(sklearn.cluster.AgglomerativeClustering(),params_dict,train_data)\n",
    "    evaluation_Score(true_labels,predicted_labels,output_df,'Agglomerative clustering')\n",
    "    output_df.loc['Agglomerative clustering','n_clusters']=len(set(predicted_labels))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Birch\")\n",
    "    params_dict={'threshold':[0.5,0.2,0.8], 'branching_factor':[50,100,200], 'n_clusters':n_clusters}\n",
    "   # params_dict={'threshold':[0.5], 'branching_factor':[50], 'n_clusters':[2]}\n",
    "    predicted_labels=runGridSearch(sklearn.cluster.Birch(),params_dict,train_data)\n",
    "    evaluation_Score(true_labels,predicted_labels,output_df,'Birch')\n",
    "    output_df.loc['Birch','n_clusters']=len(set(predicted_labels))\n",
    "    \n",
    "    \n",
    "   \n",
    "    print(\"DBSCAN\")\n",
    "    epsilons=getEpsilon(train_data)\n",
    "    params_dict = {'eps':epsilons,'min_samples':[20,30,40],'metric':['euclidean','manhattan','mahalanobis', 'minkowski']}\n",
    "\n",
    "   # params_dict = {'eps':[0.5],'min_samples':[20]}\n",
    "    predicted_labels=runGridSearch(sklearn.cluster.DBSCAN(),params_dict,train_data)\n",
    "    evaluation_Score(true_labels,predicted_labels,output_df,'DBSCAN')\n",
    "    output_df.loc['DBSCAN','n_clusters']=len(set(predicted_labels))\n",
    "    \n",
    "    \n",
    "    print(\"Mean shift\")\n",
    "    quantiles=[0.2,0.5,0.8,1]\n",
    "    params_dict={}\n",
    "    params_dict['bandwidth']=[]\n",
    "    for quantile in quantiles:\n",
    "        params_dict['bandwidth'].append(sklearn.cluster.estimate_bandwidth(train_data, quantile=quantile, n_samples=500))\n",
    "\n",
    "    predicted_labels=runGridSearch(sklearn.cluster.MeanShift(),params_dict,train_data)\n",
    "    evaluation_Score(true_labels,predicted_labels,output_df,'Mean-shift')\n",
    "    output_df.loc['Mean-shift','n_clusters']=len(set(predicted_labels))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Optics\")\n",
    "   # params_dict={'eps':[0.5],'min_samples':[20]}\n",
    "    params_dict = {'eps':[0.5,0.6,0.8],'min_samples':[20,30,40],'metric':['euclidean','manhattan','mahalanobis', 'minkowski']}\n",
    "    predicted_labels=runGridSearch(sklearn.cluster.OPTICS(),params_dict,train_data)\n",
    "    evaluation_Score(true_labels,predicted_labels,output_df,'Optics')\n",
    "    output_df.loc['Optics','n_clusters']=len(set(predicted_labels))\n",
    "    \n",
    "    \n",
    "    print(\"Gaussian Mixture\")\n",
    "    #params_dict={'covariance_type':['full'], 'max_iter':[100],'n_components':[2]}\n",
    "    params_dict={'covariance_type':['full','tied','diag','spherical'], 'max_iter':[1000000,1000,10000],'n_components':n_clusters}\n",
    "    predicted_labels=runGridSearch(sklearn.mixture.GaussianMixture(),params_dict,train_data)\n",
    "    evaluation_Score(true_labels,predicted_labels,output_df,'Gaussian-mixture')\n",
    "    output_df.loc['Gaussian-mixture','n_clusters']=len(set(predicted_labels))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(\"Spectral\")\n",
    "    #yhat,clusters=spectral_model(train_data,2)\n",
    "    #output_df=pred_cluster_label(yhat,clusters,cluster_df,output_df,'Spectral-clustering')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return output_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform(train_data):\n",
    "    pca = PCA(n_components = 2) \n",
    "    X_principal = pca.fit_transform(train_data) \n",
    "    X_principal = pd.DataFrame(X_principal) \n",
    "    X_principal.columns = ['P1', 'P2'] \n",
    "    return X_principal\n",
    "    \n",
    "\n",
    "def tsne_transform(train_data):\n",
    "    tsne = TSNE(n_components=2)\n",
    "    X_principal=tsne.fit_transform(train_data)\n",
    "    X_principal = pd.DataFrame(X_principal) \n",
    "    X_principal.columns = ['P1', 'P2'] \n",
    "    return X_principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runClustering(filename,dataset_name,dimensionality=None):\n",
    "    train_data,true_labels=load_dataset(filename,dataset_name)\n",
    "    \n",
    "    \n",
    "    output_df = pd.DataFrame(index=['K-Means','Agglomerative clustering','Birch','DBSCAN','Mean-shift','Optics','Gaussian-mixture'],columns=['n_clusters','ARI','MI','H','C','V','FM','A','R','P'])\n",
    "    \n",
    "    if(dimensionality==None):\n",
    "        output_df=runModels(train_data,true_labels,output_df,dataset_name)\n",
    "    \n",
    "    elif(dimensionality=='pca'):\n",
    "        train_data_transform=pca_transform(train_data)\n",
    "        output_df=runModels(train_data_transform,true_labels,output_df,dataset_name)\n",
    "    \n",
    "    elif(dimensionality=='tsne'):\n",
    "        train_data_transform=tsne_transform(train_data)\n",
    "        output_df=runModels(train_data_transform,true_labels,output_df,dataset_name)\n",
    "   \n",
    "    return output_df\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    files=list(os.listdir('Audio_features/'))\n",
    "    for file in files:\n",
    "        splited_file=file.split('_')\n",
    "        if(splited_file[0]!='mfcc0'):\n",
    "            print(file)\n",
    "            output_df=runClustering(os.path.join('Audio_features',file),splited_file[2],None)\n",
    "            \n",
    "            filename=\"_\".join(splited_file[0:-1])+'.csv'\n",
    "            output_df.to_csv(os.path.join('results',filename))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "           \n",
    "            filename=(\"_\".join(splited_file[0:-1])+'pca'+'.csv'\n",
    "            print(filename)\n",
    "            output_df.to_csv(os.path.join('results',filename),index=False)\n",
    "                      \n",
    "           # output_df=runClustering(os.path.join('Audio_features',file),splited_file[2],'tsne')\n",
    "            filename=(\"_\".join(splited_file[0:-1])+'tsne'+'.csv'\n",
    "            print(filename)\n",
    "            output_df.to_csv(os.path.join('results',filename),index=False)\n",
    "            \n",
    "            \"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
