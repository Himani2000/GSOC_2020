{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will do the preprocessing in the images and then apply the necessary feature extraction technique\n",
    "After that the clustering models are applied "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "from tensorflow.keras.applications.densenet import DenseNet169\n",
    "from tensorflow.keras.applications.densenet import DenseNet201\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "from tensorflow.keras.applications.nasnet import NASNetLarge\n",
    "from tensorflow.keras.applications.nasnet import NASNetMobile\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN,OPTICS,MeanShift\n",
    "import os, shutil, glob, os.path\n",
    "from PIL import Image as pil_image\n",
    "from matplotlib import pyplot as plt\n",
    "from pylab import *\n",
    "\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is adaptation more than creation. A model is not created from scratch but a pre-trained model is just adapted to a new problem. Given a small dataset which is not sufficient to build a DL model from scratch, then transfer learning is the option to automatically extract the features, we take the advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset. We can in general extract the features using the following two cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Extracting the features from the image using the pretrained models , performing the pooling operation to the output of the \n",
    "last convolution layer . See the model_vgg_16.summary() the (None,None,512) is pooled ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPretrainedWeights():\n",
    "    pretrained_weights={}\n",
    "\n",
    "    pretrained_weights['vgg16']=VGG16(weights='imagenet', include_top=False,pooling='avg')\n",
    "    pretrained_weights['vgg19']=VGG19(weights='imagenet', include_top=False,pooling='avg')\n",
    "\n",
    "    pretrained_weights['resnet50']=ResNet50(weights='imagenet', include_top=False,pooling='avg')\n",
    "\n",
    "    pretrained_weights['inceptionv3']=InceptionV3(weights='imagenet', include_top=False,pooling='avg')\n",
    "    pretrained_weights['inception-resentv2']=InceptionResNetV2(weights='imagenet', include_top=False,pooling='avg')\n",
    "\n",
    "\n",
    "    pretrained_weights['xception']=Xception(weights='imagenet', include_top=False,pooling='avg')\n",
    "\n",
    "    pretrained_weights['densenet121']=DenseNet121(weights='imagenet', include_top=False,pooling='avg')\n",
    "    pretrained_weights['densenet169']=DenseNet169(weights='imagenet', include_top=False,pooling='avg')\n",
    "    pretrained_weights['densenet201']=DenseNet201(weights='imagenet', include_top=False,pooling='avg')\n",
    "\n",
    "\n",
    "  #N retrained_weights['nasnetlarge']=NASNetLarge(weights='imagenet', include_top=False,pooling='avg',input_shape = (224, 224, 3))\n",
    "  #N pretrained_weights['nasnetmobile']=NASNetMobile(weights='imagenet', include_top=False,pooling='avg')\n",
    "\n",
    "\n",
    "\n",
    "    pretrained_weights['mobilenet']=MobileNet(weights='imagenet', include_top=False,pooling='avg')\n",
    "  #N  pretrained_weights['mobilenetV2']=MobileNetV2(weights='imagenet', include_top=False,pooling='avg')\n",
    "    \n",
    "    return pretrained_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hxn147/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights=loadPretrainedWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " pretrained_weights['vgg16'].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last few layers of the VGG16(for example) model are fully connected layers prior to the output layer. These layers will provide a complex set of features to describe a given input image and may provide useful input when training a new model for image classification or related computer vision task."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# model_vgg_16_fcn = VGG16()\n",
    "\n",
    "model_vgg_16_fcn.layers.pop()\n",
    "model_vgg_16_fcn= Model(inputs=model_vgg_16_fcn.inputs, outputs=model_vgg_16_fcn.layers[-2].output)\n",
    "\n",
    "\n",
    "model_vgg_19_fcn=VGG19()\n",
    "model_vgg_19_fcn.layers.pop()\n",
    "model_vgg_19_fcn= Model(inputs=model_vgg_19_fcn.inputs, outputs=model_vgg_19_fcn.layers[-2].output)\n",
    "\n",
    "\n",
    "model_resnet50_fcn=ResNet50()\n",
    "model_resnet50_fcn.layers.pop()\n",
    "model_resnet50_fcn= Model(inputs=model_resnet50_fcn.inputs, outputs=model_resnet50_fcn.layers[-2].output)\n",
    "\n",
    "\n",
    "model_inceptionV3_fcn=InceptionV3()\n",
    "model_inceptionV3_fcn.layers.pop()\n",
    "model_inceptionV3_fcn= Model(inputs=model_inceptionV3_fcn.inputs, outputs=model_inceptionV3_fcn.layers[-2].output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 139,570,240\n",
      "Trainable params: 139,570,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_vgg_19_fcn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the filepaths here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFilePaths(image_directory):\n",
    "    \n",
    "    files=os.listdir(image_directory)\n",
    "    files_path=[os.path.join(image_directory,file) for file in files ]\n",
    "    return files_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeDataset2(files_path):\n",
    "    subplots_adjust(hspace=0.000)\n",
    "    number_of_subplots=3\n",
    "    for i,v in enumerate(range(len(files_path))):\n",
    "        v = v+1\n",
    "        image = pil_image.open(files_path[i])\n",
    "        \n",
    "        ax1 = subplot(len(files_path),3,v)\n",
    "        \n",
    "        ax1.axis('off')\n",
    "        #ax1.figure.set_size_inches(10,15)\n",
    "        ax1.imshow(image, cmap=\"gray\", aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeDataset1(file_paths):\n",
    "    rows=2\n",
    "    for num, x in tqdm(enumerate(file_paths[0:12])):\n",
    "            img = pil_image.open(x)\n",
    "            plt.subplot(rows,6,num+1)\n",
    "            #plt.title(x.split('.')[0])\n",
    "            plt.axis('off')\n",
    "            plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting the features out of a pretrained model , features from each pretrained model is extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " # the function is used to calculate the features from the image \n",
    "def getFeatures(filelist,model): \n",
    "    filelist.sort()\n",
    "    featurelist = []\n",
    "    for i, imagepath in enumerate(filelist):\n",
    "    #for i in tqdm(range(len(filelist))):\n",
    "        print(\" Status: %s / %s\" %(i, len(filelist)), end=\"\\r\")\n",
    "        img = image.load_img(filelist[i], target_size=(224, 224))\n",
    "        img_data = image.img_to_array(img)\n",
    "        img_data = np.expand_dims(img_data, axis=0)\n",
    "        img_data = preprocess_input(img_data)\n",
    "        features = np.array(model.predict(img_data))\n",
    "        featurelist.append(features.flatten())\n",
    "    return featurelist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFeatures(features,modelname,filename):\n",
    "    saved_filename=filename+'_'+modelname\n",
    "    saved_filename=os.path.join('Image_features',saved_filename)\n",
    "\n",
    "    print(\"saving\",saved_filename+'.npy')\n",
    "    np.save(saved_filename+'.npy',features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideology_files_path=loadFilePaths('ideology_image_dataset/')\n",
    "#muslim_files_path=loadFilePaths('muslim_image_dataset/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting features for --> inception-resentv2\n",
      "saving Image_features/ideology_model_inception-resentv2.npy\n",
      "extracting features for --> xception\n",
      "saving Image_features/ideology_model_xception.npy\n",
      "extracting features for --> densenet121\n",
      "saving Image_features/ideology_model_densenet121.npy\n",
      "extracting features for --> densenet169\n",
      "saving Image_features/ideology_model_densenet169.npy\n",
      "extracting features for --> densenet201\n",
      "saving Image_features/ideology_model_densenet201.npy\n",
      "extracting features for --> mobilenet\n",
      "saving Image_features/ideology_model_mobilenet.npy\n"
     ]
    }
   ],
   "source": [
    "for model in pretrained_weights.keys():\n",
    "   # if(model!=vgg16 or model!=vgg19 or model!=resnet50 or model!=inceptionv3):\n",
    "    if(model not  in ['vgg16','vgg19','resnet50','inceptionv3']):    \n",
    "        print(\"extracting features for -->\",model)\n",
    "        ideology_features=getFeatures(ideology_files_path,pretrained_weights[model])\n",
    "        saveFeatures(ideology_features,f'model_{model}','ideology')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image_features/ideology_model_vgg_16.npy\n",
      "(2942, 512) ideology_model_vgg_16.npy\n",
      "Image_features/ideology_model_vgg_19.npy\n",
      "(2942, 512) ideology_model_vgg_19.npy\n",
      "Image_features/ideology_model_resnet50.npy\n",
      "(2942, 2048) ideology_model_resnet50.npy\n",
      "Image_features/ideology_model_inceptionv3.npy\n",
      "(2942, 2048) ideology_model_inceptionv3.npy\n",
      "Image_features/ideology_model_inception-resentv2.npy\n",
      "(2942, 1536) ideology_model_inception-resentv2.npy\n",
      "Image_features/ideology_model_xception.npy\n",
      "(2942, 2048) ideology_model_xception.npy\n",
      "Image_features/ideology_model_densenet121.npy\n",
      "(2942, 1024) ideology_model_densenet121.npy\n",
      "Image_features/ideology_model_densenet169.npy\n",
      "(2942, 1664) ideology_model_densenet169.npy\n",
      "Image_features/ideology_model_densenet201.npy\n",
      "(2942, 1920) ideology_model_densenet201.npy\n",
      "Image_features/ideology_model_mobilenet.npy\n",
      "(2942, 1024) ideology_model_mobilenet.npy\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('Image_features/'):\n",
    "    if(file.endswith('.npy')):\n",
    "        print(f'Image_features/{file}')\n",
    "        features=np.load(f'Image_features/{file}')\n",
    "        print(features.shape,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
